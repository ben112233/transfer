class BetaBandit:
    def __init__(self, n_arms, n_features):
        # For each arm and each objective, we maintain parameters for a Beta distribution
        # We can parameterize it with alpha (successes) and beta (failures)
        self.alpha = np.ones((n_arms, 3))  # 3 objectives
        self.beta = np.ones((n_arms, 3))   # 3 objectives
        
        # We can also have weights for contextual features for each arm and objective
        self.weights = np.zeros((n_arms, 3, n_features))

    def update(self, context, arm, rewards):
        """Update parameters for the chosen arm based on observed rewards"""
        for obj in range(3):
            # Update the prior Beta distribution for this arm/objective
            # This can be a simple Bayesian update or a more complex Beta regression
            self.alpha[arm, obj] += rewards[obj] * some_function_of(context)
            self.beta[arm, obj] += (1 - rewards[obj]) * some_function_of(context)
            
            # Alternatively, update contextual weights for Beta regression
            # (This is more complex to implement)

    def expected_rewards(self, context):
        """Get the expected value for each arm and objective"""
        expected = np.zeros((self.n_arms, 3))
        for arm in range(self.n_arms):
            for obj in range(3):
                # Mean of Beta(alpha, beta) is alpha / (alpha + beta)
                expected[arm, obj] = self.alpha[arm, obj] / (self.alpha[arm, obj] + self.beta[arm, obj])
        return expected

    def choose_arm(self, context):
        """Use Thompson Sampling by drawing samples from the Beta posteriors"""
        samples = np.zeros((self.n_arms, 3))
        for arm in range(self.n_arms):
            for obj in range(3):
                samples[arm, obj] = np.random.beta(self.alpha[arm, obj], self.beta[arm, obj])
        
        # Combine the 3 objective samples into a single score (e.g., weighted sum)
        scores = np.average(samples, axis=1, weights=[w1, w2, w3])
        return np.argmax(scores)
