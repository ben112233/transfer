import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

class MultiObjectiveLinearTS:
    """Multi-Objective Linear Thompson Sampling Bandit"""
    
    def __init__(self, n_arms, n_features, n_objectives, alpha=1.0):
        self.n_arms = n_arms
        self.n_features = n_features
        self.n_objectives = n_objectives
        self.alpha = alpha  # Exploration parameter
        
        # Initialize parameters for each arm and each objective
        self.B = np.array([np.eye(n_features) for _ in range(n_arms) for _ in range(n_objectives)]).reshape(n_arms, n_objectives, n_features, n_features)
        self.mu = np.zeros((n_arms, n_objectives, n_features))
        self.f = np.zeros((n_arms, n_objectives, n_features))
        
        # Keep track of rewards for each objective
        self.total_rewards = np.zeros((n_arms, n_objectives))
        self.counts = np.zeros(n_arms)
    
    def choice(self, context):
        """Choose an arm using Thompson Sampling"""
        if np.random.random() < 0.1:  # 10% exploration
            return np.random.randint(self.n_arms)
        
        # Sample from posterior for each arm and each objective
        sampled_rewards = np.zeros((self.n_arms, self.n_objectives))
        
        for arm in range(self.n_arms):
            for obj in range(self.n_objectives):
                # Sample from multivariate normal
                theta_sample = np.random.multivariate_normal(
                    self.mu[arm, obj], 
                    self.alpha * np.linalg.inv(self.B[arm, obj])
                )
                sampled_rewards[arm, obj] = context @ theta_sample
        
        # Convert to scalar using a simple weighted sum (could use more sophisticated methods)
        # Weights: engagement=0.3, conversion=0.4, profit=0.3
        weights = np.array([0.3, 0.4, 0.3])[:self.n_objectives]
        scalar_rewards = sampled_rewards @ weights
        
        return np.argmax(scalar_rewards)
    
    def update(self, context, arm, reward):
        """Update the bandit parameters"""
        self.counts[arm] += 1
        self.total_rewards[arm] += reward
        
        for obj in range(self.n_objectives):
            # Update Bayesian parameters
            self.B[arm, obj] += np.outer(context, context)
            self.f[arm, obj] += reward[obj] * context
            self.mu[arm, obj] = np.linalg.inv(self.B[arm, obj]) @ self.f[arm, obj]
    
    def expected_rewards(self, context):
        """Get expected rewards for all arms"""
        expected = np.zeros((self.n_arms, self.n_objectives))
        for arm in range(self.n_arms):
            for obj in range(self.n_objectives):
                expected[arm, obj] = context @ self.mu[arm, obj]
        return expected

# 1. Create a synthetic banking dataset
def create_banking_dataset(n_samples=5000):
    """Create a realistic synthetic banking dataset"""
    np.random.seed(42)
    
    data = {
        'age': np.random.normal(45, 15, n_samples).clip(18, 80),
        'income': np.random.lognormal(10.5, 0.8, n_samples).clip(20000, 200000),
        'credit_score': np.random.normal(700, 100, n_samples).clip(300, 850),
        'num_products': np.random.poisson(1.5, n_samples).clip(0, 5),
        'savings_balance': np.random.exponential(5000, n_samples),
        'checking_balance': np.random.exponential(3000, n_samples),
        'days_since_last_login': np.random.exponential(7, n_samples),
        'has_mortgage': np.random.binomial(1, 0.3, n_samples),
        'is_premium_customer': np.random.binomial(1, 0.2, n_samples)
    }
    
    df = pd.DataFrame(data)
    
    # Create some realistic relationships
    df['credit_score'] = df['credit_score'] + (df['income'] / 10000) * 5
    df['savings_balance'] = df['savings_balance'] * (df['income'] / 50000)
    
    return df

# 2. Define the true reward functions
def calculate_true_rewards(user_features, arm):
    """
    Calculate true rewards for a user and product arm
    Returns: [engagement, conversion, profit]
    """
    age, income, credit_score, num_products = user_features[:4]
    
    # Base probabilities and values
    if arm == 0:  # Basic Savings Account
        engage_prob = 0.6 + 0.001 * income/1000
        convert_prob = 0.4 + 0.0005 * income/1000
        profit = 0.1 + 0.00001 * income
        
    elif arm == 1:  # Premium Credit Card
        engage_prob = 0.3 + 0.002 * income/1000 + 0.001 * credit_score
        convert_prob = 0.2 + 0.0015 * income/1000 + 0.002 * credit_score
        profit = 0.7 + 0.00005 * income + 0.001 * credit_score
        
    elif arm == 2:  # Home Loan Pre-approval
        engage_prob = 0.2 + 0.003 * income/1000 + 0.001 * age
        convert_prob = 0.1 + 0.002 * income/1000 + 0.002 * age
        profit = 0.9 + 0.0001 * income + 0.002 * age
        
    else:  # arm == 3: Retirement Plan
        engage_prob = 0.4 + 0.002 * age + 0.001 * income/1000
        convert_prob = 0.3 + 0.0015 * age + 0.001 * income/1000
        profit = 0.5 + 0.00003 * income + 0.0015 * age
    
    # Add some noise
    engage_prob = np.clip(engage_prob + np.random.normal(0, 0.1), 0.1, 0.95)
    convert_prob = np.clip(convert_prob + np.random.normal(0, 0.08), 0.05, 0.9)
    profit = np.clip(profit + np.random.normal(0, 0.05), 0.05, 1.2)
    
    # Simulate binary outcomes for engagement and conversion
    engagement = 1 if np.random.random() < engage_prob else 0
    conversion = 1 if np.random.random() < convert_prob else 0
    
    return np.array([engagement, conversion, profit])

# 3. Create and prepare the dataset
print("Creating synthetic banking dataset...")
df = create_banking_dataset(3000)
print(f"Dataset shape: {df.shape}")

# Feature columns
feature_columns = ['age', 'income', 'credit_score', 'num_products', 
                  'savings_balance', 'checking_balance', 'days_since_last_login',
                  'has_mortgage', 'is_premium_customer']

# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[feature_columns])

# 4. Initialize the Multi-Objective Bandit
n_arms = 4  # 4 banking products
n_features = len(feature_columns)
n_objectives = 3  # engagement, conversion, profit

bandit = MultiObjectiveLinearTS(n_arms, n_features, n_objectives, alpha=1.0)

# 5. Train the bandit
print("\nTraining bandit on historical data...")
historical_rewards = []

for i in range(len(df)):
    if i % 500 == 0:
        print(f"Processing sample {i}/{len(df)}")
    
    context = X_scaled[i]
    
    # Mixed strategy for historical training
    if np.random.random() < 0.7:
        # Some business rules
        user_income = df['income'].iloc[i]
        user_credit = df['credit_score'].iloc[i]
        user_age = df['age'].iloc[i]
        
        if user_income > 100000 and user_credit > 750:
            arm = np.random.choice([1, 2])  # Premium products
        elif user_age > 50:
            arm = np.random.choice([0, 3])  # Savings or Retirement
        else:
            arm = np.random.choice([0, 1])  # Basic products
    else:
        arm = np.random.randint(n_arms)
    
    reward = calculate_true_rewards(df[feature_columns].iloc[i].values, arm)
    historical_rewards.append(reward)
    
    bandit.update(context, arm, reward)

print("Training completed!")

# 6. Evaluate the bandit
def evaluate_bandit(bandit, test_df, scaler, feature_columns, n_trials=100):
    """Evaluate the bandit on test data"""
    test_rewards = []
    X_test = scaler.transform(test_df[feature_columns])
    product_names = ['Savings Account', 'Credit Card', 'Home Loan', 'Retirement Plan']
    
    for i in range(min(n_trials, len(test_df))):
        context = X_test[i]
        user_data = test_df.iloc[i]
        
        recommended_arm = bandit.choice(context)
        reward = calculate_true_rewards(user_data[feature_columns].values, recommended_arm)
        test_rewards.append(reward)
        
        if i < 3:
            print(f"User {i}: Age={user_data['age']:.0f}, Income=${user_data['income']:.0f}, "
                  f"Credit={user_data['credit_score']:.0f} -> "
                  f"Recommended: {product_names[recommended_arm]} -> "
                  f"Reward: Engagement={reward[0]}, Conversion={reward[1]}, Profit={reward[2]:.2f}")
    
    test_rewards = np.array(test_rewards)
    avg_rewards = test_rewards.mean(axis=0)
    
    print(f"\nAverage Test Performance ({n_trials} users):")
    print(f"Engagement Rate: {avg_rewards[0]:.3f}")
    print(f"Conversion Rate: {avg_rewards[1]:.3f}")
    print(f"Average Profit: {avg_rewards[2]:.3f}")
    
    return test_rewards

# Split data
train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)
print(f"\nEvaluation on test set ({len(test_df)} users)...")
test_rewards = evaluate_bandit(bandit, test_df, scaler, feature_columns, n_trials=150)

# 7. Make recommendations for new users
def recommend_product(user_data, bandit, scaler, feature_columns):
    """Make a product recommendation for a new user"""
    user_df = pd.DataFrame([user_data])
    context = scaler.transform(user_df[feature_columns])[0]
    
    recommended_arm = bandit.choice(context)
    expected_rewards = bandit.expected_rewards(context)
    
    products = ['Basic Savings', 'Premium Credit Card', 'Home Loan', 'Retirement Plan']
    
    print(f"\nRecommendation for user:")
    print(f"Age: {user_data['age']}, Income: ${user_data['income']:.0f}, "
          f"Credit Score: {user_data['credit_score']}")
    print(f"Recommended product: {products[recommended_arm]}")
    
    print("\nExpected outcomes for all products:")
    for i, product in enumerate(products):
        eng_prob = 1 / (1 + np.exp(-expected_rewards[i][0]))  # Sigmoid for probability
        conv_prob = 1 / (1 + np.exp(-expected_rewards[i][1]))
        profit = expected_rewards[i][2]
        print(f"{product}: Engagement={eng_prob:.3f}, Conversion={conv_prob:.3f}, Profit={profit:.3f}")
    
    return recommended_arm

# Example new users
new_users = [
    {'age': 28, 'income': 85000, 'credit_score': 720, 'num_products': 2,
     'savings_balance': 12000, 'checking_balance': 5000, 'days_since_last_login': 2,
     'has_mortgage': 0, 'is_premium_customer': 0},
    
    {'age': 55, 'income': 145000, 'credit_score': 780, 'num_products': 3,
     'savings_balance': 85000, 'checking_balance': 15000, 'days_since_last_login': 5,
     'has_mortgage': 1, 'is_premium_customer': 1},
    
    {'age': 35, 'income': 55000, 'credit_score': 650, 'num_products': 1,
     'savings_balance': 8000, 'checking_balance': 3000, 'days_since_last_login': 15,
     'has_mortgage': 0, 'is_premium_customer': 0}
]

print("\n" + "="*60)
print("PRODUCT RECOMMENDATIONS")
print("="*60)

for i, user in enumerate(new_users):
    print(f"\n--- User {i+1} ---")
    recommend_product(user, bandit, scaler, feature_columns)

# 8. Performance analysis
print("\n" + "="*60)
print("PERFORMANCE ANALYSIS")
print("="*60)

# Compare with random strategy
def compare_with_random(test_df, n_trials=100):
    bandit_rewards = []
    random_rewards = []
    
    X_test = scaler.transform(test_df[feature_columns].iloc[:n_trials])
    
    for i in range(n_trials):
        context = X_test[i]
        user_data = test_df.iloc[i]
        
        # Bandit choice
        bandit_arm = bandit.choice(context)
        bandit_reward = calculate_true_rewards(user_data[feature_columns].values, bandit_arm)
        bandit_rewards.append(bandit_reward)
        
        # Random choice
        random_arm = np.random.randint(n_arms)
        random_reward = calculate_true_rewards(user_data[feature_columns].values, random_arm)
        random_rewards.append(random_reward)
    
    bandit_rewards = np.array(bandit_rewards)
    random_rewards = np.array(random_rewards)
    
    print("Comparison with Random Strategy:")
    print(f"{'Metric':<12} {'Bandit':<8} {'Random':<8} {'Improvement':<12}")
    print("-" * 40)
    
    metrics = ['Engagement', 'Conversion', 'Profit']
    for j, metric in enumerate(metrics):
        bandit_avg = bandit_rewards[:, j].mean()
        random_avg = random_rewards[:, j].mean()
        improvement = ((bandit_avg - random_avg) / random_avg) * 100
        print(f"{metric:<12} {bandit_avg:.3f}    {random_avg:.3f}    {improvement:+.1f}%")

compare_with_random(test_df)

# 9. Plot learning progress
plt.figure(figsize=(12, 8))

# Cumulative rewards
cumulative_rewards = np.array(historical_rewards).cumsum(axis=0)

plt.subplot(2, 2, 1)
plt.plot(cumulative_rewards[:, 0], label='Engagement')
plt.plot(cumulative_rewards[:, 1], label='Conversion')
plt.plot(cumulative_rewards[:, 2], label='Profit')
plt.xlabel('Users')
plt.ylabel('Cumulative Reward')
plt.title('Cumulative Rewards Over Time')
plt.legend()

# Moving averages
window = 100
moving_avg = pd.DataFrame(historical_rewards).rolling(window=window).mean()

plt.subplot(2, 2, 2)
plt.plot(moving_avg[0], label='Engagement')
plt.plot(moving_avg[1], label='Conversion')
plt.plot(moving_avg[2], label='Profit')
plt.xlabel('Users')
plt.ylabel(f'Moving Average ({window} users)')
plt.title('Performance Moving Average')
plt.legend()

# Arm selection counts
plt.subplot(2, 2, 3)
arm_counts = [np.sum([historical_rewards[i] for i in range(len(historical_rewards)) 
                     if np.argmax(bandit.expected_rewards(X_scaled[i]) @ [0.3, 0.4, 0.3]) == arm], axis=0) 
              for arm in range(n_arms)]

products = ['Savings', 'Credit Card', 'Home Loan', 'Retirement']
x = np.arange(n_arms)
width = 0.25

plt.bar(x - width, [count[0] for count in arm_counts], width, label='Engagement')
plt.bar(x, [count[1] for count in arm_counts], width, label='Conversion')
plt.bar(x + width, [count[2] for count in arm_counts], width, label='Profit')
plt.xlabel('Product')
plt.ylabel('Total Reward')
plt.title('Rewards by Product Type')
plt.xticks(x, products)
plt.legend()

plt.tight_layout()
plt.show()
