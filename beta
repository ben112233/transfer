import numpy as np
import scipy.optimize
import scipy.special
from scipy.stats import beta

def calculate_true_rewards(user_features, engagement_base, revenue_increment_base, 
                          diversity_base, arm, simulate=False):
    """
    Calculate true rewards for a user and product arm
    Returns: [engagement, revenue_increment, product_diversity] bounded between [0,1]
    """
    if simulate:
        # Generate Gamma-distributed values (positive and continuous)
        engagement = np.random.gamma(2, engagement_base[str(arm)+'.mean']/2)
        revenue_increment = np.random.gamma(2, revenue_increment_base[str(arm)+'.mean']/2)
        product_diversity = np.random.gamma(2, diversity_base[str(arm)+'.mean']/2)
        
        # Convert to [0,1] range using logistic function for Beta regression compatibility
        # This ensures rewards are properly bounded for Beta distribution
        engagement = 1.0 / (1.0 + np.exp(-engagement))
        revenue_increment = 1.0 / (1.0 + np.exp(-revenue_increment))
        product_diversity = 1.0 / (1.0 + np.exp(-product_diversity))
        
        # Add some noise and ensure within bounds
        engagement = np.clip(engagement + np.random.normal(0, 0.05), 0.01, 0.99)
        revenue_increment = np.clip(revenue_increment + np.random.normal(0, 0.05), 0.01, 0.99)
        product_diversity = np.clip(product_diversity + np.random.normal(0, 0.05), 0.01, 0.99)
    else:  
        # Use direct features (assuming they're already in [0,1])
        engagement = user_features[-3]  
        revenue_increment = user_features[-2]  
        product_diversity = user_features[-1]
    
    return [engagement, revenue_increment, product_diversity]

class MultiObjectiveBetaBandit:
    def __init__(self, n_arms, n_features, n_objectives=3, prior_alpha=2.0, prior_beta=2.0):
        """
        Multi-objective contextual bandit using Beta regression
        """
        self.n_arms = n_arms
        self.n_features = n_features
        self.n_objectives = n_objectives
        
        # Prior parameters for Beta distribution
        self.alpha = np.ones((n_arms, n_objectives)) * prior_alpha
        self.beta = np.ones((n_arms, n_objectives)) * prior_beta
        
        # Weight vectors for contextual Beta regression
        self.weights = np.zeros((n_arms, n_objectives, n_features))
        
        # Store history for regression
        self.context_history = [[] for _ in range(n_arms)]
        self.reward_history = [[] for _ in range(n_arms)]
        self.n_pulls = np.zeros(n_arms)

    def _beta_regression_loss(self, weights, contexts, rewards, epsilon=1e-6):
        """Loss function for Beta regression with logit link"""
        # Linear predictor
        theta = np.dot(contexts, weights)
        
        # Apply logit link to get mean parameter μ ∈ (0,1)
        mu = 1.0 / (1.0 + np.exp(-theta))
        mu = np.clip(mu, epsilon, 1.0 - epsilon)
        
        # Fixed precision parameter
        k = 20.0
        alpha_param = mu * k
        beta_param = (1.0 - mu) * k
        
        # Beta log-likelihood
        log_likelihood = ((alpha_param - 1) * np.log(rewards + epsilon) + 
                         (beta_param - 1) * np.log(1.0 - rewards + epsilon) -
                         scipy.special.betaln(alpha_param, beta_param))
        
        return -np.sum(log_likelihood)

    def update(self, context, arm, reward_vector):
        """Update bandit parameters with new observation"""
        # Store context and reward
        self.context_history[arm].append(context)
        self.reward_history[arm].append(reward_vector)
        self.n_pulls[arm] += 1
        
        # Update Beta parameters (simple Bayesian update)
        for obj in range(self.n_objectives):
            reward = reward_vector[obj]
            self.alpha[arm, obj] += reward
            self.beta[arm, obj] += (1.0 - reward)
            
            # Update regression weights if we have enough data
            if self.n_pulls[arm] > self.n_features:
                contexts = np.array(self.context_history[arm])
                rewards = np.array([r[obj] for r in self.reward_history[arm]])
                
                try:
                    result = scipy.optimize.minimize(
                        self._beta_regression_loss,
                        self.weights[arm, obj],
                        args=(contexts, rewards),
                        method='L-BFGS-B',
                        bounds=[(-3, 3)] * self.n_features,
                        options={'maxiter': 50}
                    )
                    if result.success:
                        self.weights[arm, obj] = result.x
                except:
                    pass

    def expected_rewards(self, context):
        """Get expected rewards for all arms and objectives"""
        expected = np.zeros((self.n_arms, self.n_objectives))
        
        for arm in range(self.n_arms):
            for obj in range(self.n_objectives):
                if self.n_pulls[arm] > self.n_features:
                    # Use regression prediction
                    theta = np.dot(context, self.weights[arm, obj])
                    mu = 1.0 / (1.0 + np.exp(-theta))
                    expected[arm, obj] = mu
                else:
                    # Use Beta mean
                    expected[arm, obj] = self.alpha[arm, obj] / (self.alpha[arm, obj] + self.beta[arm, obj])
        
        return np.clip(expected, 0.01, 0.99)

    def choose_arm(self, context, method="thompson", objective_weights=None):
        """Choose arm using Thompson sampling or expected values"""
        if objective_weights is None:
            objective_weights = np.ones(self.n_objectives) / self.n_objectives
        
        if method == "thompson":
            # Sample from posterior Beta distributions
            samples = np.zeros((self.n_arms, self.n_objectives))
            for arm in range(self.n_arms):
                for obj in range(self.n_objectives):
                    samples[arm, obj] = np.random.beta(self.alpha[arm, obj], self.beta[arm, obj])
            
            # Weight objectives and get scores
            scores = np.dot(samples, objective_weights)
        else:
            # Use expected values
            expected = self.expected_rewards(context)
            scores = np.dot(expected, objective_weights)
        
        return np.argmax(scores)

# Example usage with your reward function
def simulate_bandit_experiment():
    # Initialize bandit and reward parameters
    n_arms = 4
    n_features = 8  # 5 features + 3 reward components
    bandit = MultiObjectiveBetaBandit(n_arms, n_features)
    
    # Example base parameters (would come from your data)
    engagement_base = {'0.mean': 0.6, '0.std': 0.1, '1.mean': 0.7, '1.std': 0.1,
                      '2.mean': 0.5, '2.std': 0.2, '3.mean': 0.8, '3.std': 0.1}
    revenue_increment_base = {'0.mean': 0.4, '0.std': 0.2, '1.mean': 0.3, '1.std': 0.1,
                             '2.mean': 0.5, '2.std': 0.2, '3.mean': 0.6, '3.std': 0.1}
    diversity_base = {'0.mean': 0.7, '0.std': 0.1, '1.mean': 0.5, '1.std': 0.2,
                     '2.mean': 0.6, '2.std': 0.1, '3.mean': 0.4, '3.std': 0.2}
    
    # Simulation loop
    for i in range(1000):
        # Generate random user features (last 3 will be used as rewards in non-simulate mode)
        user_features = np.random.uniform(0, 1, n_features)
        
        # Choose arm
        arm = bandit.choose_arm(user_features[:5])  # Use first 5 features as context
        
        # Calculate rewards using your function
        rewards = calculate_true_rewards(
            user_features, engagement_base, revenue_increment_base, 
            diversity_base, arm, simulate=True
        )
        
        # Update bandit
        bandit.update(user_features[:5], arm, rewards)
        
        if i % 100 == 0:
            expected = bandit.expected_rewards(user_features[:5])
            print(f"Step {i}: Arm {arm} - Rewards: {rewards}")
            print(f"Expected: {np.mean(expected, axis=1)}")
    
    return bandit

# Run the simulation
if __name__ == "__main__":
    bandit = simulate_bandit_experiment()
