class MultiObjectiveBetaBandit:
    def __init__(self, n_arms, n_features, n_objectives=3, prior_alpha=2.0, prior_beta=2.0):
        """
        Multi-objective contextual bandit using Beta regression
        """
        self.n_arms = n_arms
        self.n_features = n_features
        self.n_objectives = n_objectives
        
        # Prior parameters for Beta distribution
        self.alpha = np.ones((n_arms, n_objectives)) * prior_alpha
        self.beta = np.ones((n_arms, n_objectives)) * prior_beta
        
        # Weight vectors for contextual Beta regression
        self.weights = np.zeros((n_arms, n_objectives, n_features))
        
        # Store history for regression
        self.context_history = [[] for _ in range(n_arms)]
        self.reward_history = [[] for _ in range(n_arms)]
        self.n_pulls = np.zeros(n_arms)
        
        # Performance tracking
        self.performance_history = []

    def get_uncertainty(self, arm, objective):
        """
        Calculate uncertainty (variance) for a specific arm and objective
        Variance of Beta(α, β) = (α * β) / ((α + β)^2 * (α + β + 1))
        """
        alpha_val = self.alpha[arm, objective]
        beta_val = self.beta[arm, objective]
        total = alpha_val + beta_val
        if total == 0:
            return 1.0  # Maximum uncertainty if no data
        return (alpha_val * beta_val) / ((total ** 2) * (total + 1))

    def get_arm_uncertainty(self, arm):
        """Get average uncertainty across all objectives for an arm"""
        uncertainties = [self.get_uncertainty(arm, obj) for obj in range(self.n_objectives)]
        return np.mean(uncertainties)

    def _beta_regression_loss(self, weights, contexts, rewards, epsilon=1e-6):
        """Loss function for Beta regression with logit link"""
        # Linear predictor
        theta = np.dot(contexts, weights)
        
        # Apply logit link to get mean parameter μ ∈ (0,1)
        mu = 1.0 / (1.0 + np.exp(-theta))
        mu = np.clip(mu, epsilon, 1.0 - epsilon)
        
        # Fixed precision parameter
        k = 20.0
        alpha_param = mu * k
        beta_param = (1.0 - mu) * k
        
        # Beta log-likelihood
        log_likelihood = ((alpha_param - 1) * np.log(rewards + epsilon) + 
                         (beta_param - 1) * np.log(1.0 - rewards + epsilon) -
                         scipy.special.betaln(alpha_param, beta_param))
        
        return -np.sum(log_likelihood)

    def update(self, context, arm, reward_vector):
        """Update bandit parameters with new observation"""
        # Store context and reward
        self.context_history[arm].append(context)
        self.reward_history[arm].append(reward_vector)
        self.n_pulls[arm] += 1
        
        # Update Beta parameters (simple Bayesian update)
        for obj in range(self.n_objectives):
            reward = reward_vector[obj]
            self.alpha[arm, obj] += reward
            self.beta[arm, obj] += (1.0 - reward)
            
            # Update regression weights if we have enough data
            if self.n_pulls[arm] > self.n_features:
                contexts = np.array(self.context_history[arm])
                rewards = np.array([r[obj] for r in self.reward_history[arm]])
                
                try:
                    result = scipy.optimize.minimize(
                        self._beta_regression_loss,
                        self.weights[arm, obj],
                        args=(contexts, rewards),
                        method='L-BFGS-B',
                        bounds=[(-3, 3)] * self.n_features,
                        options={'maxiter': 50, 'disp': False}
                    )
                    if result.success:
                        self.weights[arm, obj] = result.x
                except:
                    pass
        
        # Log performance periodically
        if np.sum(self.n_pulls) % 50 == 0:
            self._log_performance()

    def _log_performance(self):
        """Monitor bandit performance"""
        performance = {
            'total_pulls': np.sum(self.n_pulls),
            'arm_pulls': self.n_pulls.copy(),
            'arm_means': [self.alpha[arm] / (self.alpha[arm] + self.beta[arm]) for arm in range(self.n_arms)],
            'arm_uncertainties': [self.get_arm_uncertainty(arm) for arm in range(self.n_arms)]
        }
        self.performance_history.append(performance)
        print(f"Performance update - Total pulls: {performance['total_pulls']}")
        for arm in range(self.n_arms):
            print(f"  Arm {arm}: pulls={performance['arm_pulls'][arm]}, "
                  f"means={performance['arm_means'][arm]}, "
                  f"uncertainty={performance['arm_uncertainties'][arm]:.3f}")

    def expected_rewards(self, context):
        """Get expected rewards for all arms and objectives"""
        expected = np.zeros((self.n_arms, self.n_objectives))
        
        for arm in range(self.n_arms):
            for obj in range(self.n_objectives):
                if self.n_pulls[arm] > max(20, self.n_features * 2):
                    # Use regression prediction
                    theta = np.dot(context, self.weights[arm, obj])
                    mu = 1.0 / (1.0 + np.exp(-theta))
                    expected[arm, obj] = mu
                else:
                    # Use Beta mean
                    expected[arm, obj] = self.alpha[arm, obj] / (self.alpha[arm, obj] + self.beta[arm, obj])
        
        return np.clip(expected, 0.01, 0.99)

    def choose_arm(self, context, method="thompson", objective_weights=None, exploration_factor=0.5):
        """Choose arm using Thompson sampling or expected values with exploration"""
        if objective_weights is None:
            objective_weights = np.ones(self.n_objectives) / self.n_objectives
        
        if method == "thompson":
            # Thompson sampling with exploration bonus
            scores = np.zeros(self.n_arms)
            
            for arm in range(self.n_arms):
                # Sample from posterior Beta distribution
                sample = np.array([np.random.beta(self.alpha[arm, obj], self.beta[arm, obj]) 
                                 for obj in range(self.n_objectives)])
                
                # Calculate base score
                base_score = np.dot(sample, objective_weights)
                
                # Add exploration bonus based on uncertainty
                uncertainty = self.get_arm_uncertainty(arm)
                exploration_bonus = exploration_factor * uncertainty
                
                scores[arm] = base_score + exploration_bonus
            
            return np.argmax(scores)
            
        else:
            # Use expected values
            expected = self.expected_rewards(context)
            scores = np.dot(expected, objective_weights)
            return np.argmax(scores)

    def get_arm_statistics(self, arm):
        """Get comprehensive statistics for an arm"""
        stats = {
            'pulls': self.n_pulls[arm],
            'means': self.alpha[arm] / (self.alpha[arm] + self.beta[arm]),
            'uncertainties': [self.get_uncertainty(arm, obj) for obj in range(self.n_objectives)],
            'alpha_params': self.alpha[arm].copy(),
            'beta_params': self.beta[arm].copy()
        }
        return stats
